---
title: "STA380 Exercises"
authors: Shawn Kalish, Jackson Hassell
date: "8/16/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

[github link](https://github.com/shawnmkalish/ML_Exercises)

# **1. Visual story telling part 1: green buildings**

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(arules)
library(arulesViz)
library(RColorBrewer)
library(ggplot2)
library(mosaic)
library(quantmod)
library(foreach)
library(plyr)
library(dplyr)
library(LICORS) # for kmeans++
library(cluster)
library(readr)
library(tm) 
library(tidyverse)
library(randomForest)
library(plotly)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

green = read.csv("C:/Users/smkal/Desktop/ML_2nd Half/data/greenbuildings.csv")

#Rent is overall higher for green buildings, but is that the whole story?
ggplot(green, aes(green_rating, Rent)) + 
  stat_summary(fun.y = mean, geom = "bar", na.rm = TRUE)
```

### It's possible green buildings are usually built in more expensive areas, and are not more expensive themselves. 

But when we graph it, we see green buildings are more expensive than their counterparts in the same cluster. If green buildings cost the same as their cluster, we would expect them to all be along that blue line, but we see most are above it.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#Rent is usually higher for green buildings, even within the same cluster. So green buildings are not just being built in more expensive places
ggplot(green[green$green_rating == 1,], aes(cluster_rent, Rent)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, color="blue") +
  ylab('Green Building Rent')
```

### Are green buildings usually nicer than non-green buildings?

When we graph it, we see green buildings are disproportionately in the highest class of buildings class_a, and non-green buildings are more likely to be class_b or class_c. They are also more likely to have renovated, and are usually newer rather than renovated. Additionally, we can see that nicer buildings cost more to rent, as you'd expect.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

green$class_c = abs((green$class_a + green$class_b) - 1) 
ggplot(green, aes(x=green_rating, y=class_a, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Percentage of Green and Non-Green Buildings in Class A')
ggplot(green, aes(x=green_rating, y=class_b, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Percentage of Green and Non-Green Buildings in Class B')
ggplot(green, aes(x=green_rating, y=class_c, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Percentage of Green and Non-Green Buildings in Class C')
ggplot(green, aes(x=green_rating, y=amenities, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Percentage of Green and Non-Green Buildings With Amenities')
ggplot(green, aes(x=green_rating, y=renovated, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Percentage of Green and Non-Green Buildings That Are Renovated')
ggplot(green, aes(x=green_rating, y=age, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Age of Green and Non-Green Buildings')


ggplot(green, aes(x=class_a, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings in Class A')
ggplot(green, aes(x=class_b, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings in Class B')
ggplot(green, aes(x=class_c, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings in Class C')
ggplot(green, aes(x=amenities, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings with Amenities')
ggplot(green, aes(x=renovated, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings That Have Been Renovated')
green$new = as.numeric(green$age < 20)
ggplot(green, aes(x=new, y=Rent)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Average Rent of Buildings Under 20 Years Old')
```

### At this stage, however, we don't know if the buildings cost more because they're more likely to be green or if they're just higher class. 

So let's compare prices between green buildings and non-green buildings while holding niceness constant.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(green[green$class_a == 1,], aes(x=green_rating, y=Rent, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Rent of Green and Non-Green Buildings in Class A')
ggplot(green[green$class_b == 1,], aes(x=green_rating, y=Rent, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Rent of Green and Non-Green Buildings in Class B')
ggplot(green[green$class_c == 1,], aes(x=green_rating, y=Rent, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Rent of Green and Non-Green Buildings in Class C')
ggplot(green[green$amenities == 1,], aes(x=green_rating, y=Rent, fill=green_rating)) + 
  geom_bar(position="dodge", stat="summary", fun='mean') +
  ggtitle('Rent of Green and Non-Green Buildings with Amenities')
```

Here we can see green buildings cost about as much as their non-green counterparts when you hold niceness constant. The only exception in class_c buildings, and considering how few green class_c buildings there are it's dangerous to draw too many conclusions from that data.

From this we can conclude that you are unlikely to make significantly more money from rent by building a green building than a non-green one.

There are still reasons to go green: it's good PR, they tend to last longer, it helps the environment, you might even save some money on utilities. But don't expect to make more rent money by going green.

# **2. Visual story telling part 2: flights at ABIA**

We can see a general trend in cancelled flights in Austin - a flight is most likely to be cancelled in the Spring and least likely to get cancelled in the fall, with a more or less smooth sine curve between those two extremes, with one notable exception. September has almost as many cancellations as April. 

### What's causing this?

```{r, echo=FALSE, message=FALSE, warning=FALSE}

abia = read.csv("C:/Users/smkal/Desktop/ML_2nd Half/data/ABIA.csv")

abia[is.na(abia)] = 0

ggplot(abia) + 
  geom_bar(aes(x=Month, y=Cancelled), position="dodge", stat="summary") +
  ggtitle('Number of Flights Being Cancelled per Month')
```

### The number of flights each month is relatively constant, so its not that September is just a particularly busy month.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(data=abia, aes(x=Month)) + 
  geom_histogram() +
  ggtitle('Number of Flights per Month')
```

### By looking at the cancellation codes, we can see that September flights are much more likely to get cancelled for weather reasons than any other month. The most likely cause of this is Hurricane Ike, which hit Galveston on September 13th, 2008.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

abia$CancellationCode[abia$CancellationCode == 'A'] = 'Carrier Cancellation'
abia$CancellationCode[abia$CancellationCode == 'B'] = 'Weather Cancellation'
abia$CancellationCode[abia$CancellationCode == 'C'] = 'NAS Cancellation'
ggplot(abia[abia$Cancelled == TRUE,]) + 
  geom_bar(aes(x=Month, y=Cancelled, fill=CancellationCode), position="fill", stat="identity") +
  ggtitle('Cancellation Codes per Month')
```

### We can confirm this by graphing cancelled flights in September over time. Below, you can see a sharp spike around the 13th, right when Ike hit.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

ggplot(abia[abia$Month == 9 & abia$Cancelled == TRUE,]) + 
  geom_bar(aes(x=DayofMonth, fill=CancellationCode), position="dodge", stat="count") +
  ggtitle('Cancellation Codes in September')
```

# **3. Portfolio Modeling**

We want to create three diverse portfolios that are not only diverse within the portfolios themselves, but also diverse compared to the other portfolios.

# *(a) Portfolio 1:------------------------------------------------------------*

## Portfolio 1 contains 3 categories: High Volatility, Low Volatility, and Diverse:

### High Volatility:
Technology Equities ETFs: These ETFs offer exposure to stocks within the technology sector. Note that tech stocks tend to carry a bit more volatility than other sectors, as they have a higher risk/reward profile.

* VGT - Vanguard Information Technology ETF
* SOXX - iShares Semiconductor ETF

### Low Volatility:
Large Cap Value Equities ETFs: These ETFs offer exposure to domestic large cap size securities deemed to possess value characteristics. These types of securities are generally in stable industries with low to moderate growth prospects and trade at relative low price-to-earnings ratios. As such, value equities tend to be more appealing to income-focused investors rather than those who are entirely interested in capital appreciation.

* VLUE - iShares MSCI USA Value Factor  ETF
* HDV - iShares Core High Dividend ETF

### Diverse:
Diversified Portfolios ETFs: These ETFs offer investors exposure to multiple asset classes through a single ticker. These funds vary in investment objectives and risk/return profiles, but typically invest in a mix of equities and fixed income securities. Some diversified portfolio ETFs also offer exposure to commodity and currency exposure as well.

* AOA - iShares Core Aggressive Allocation ETF

```{r Portfolio 1 Bootstrap, echo = FALSE,  warning = FALSE, message = FALSE}

my_etfs1 = c("VGT", "SOXX", "VLUE", "HDV", "AOA")
my_prices_1 = getSymbols(my_etfs1, from = "2016-08-16")

# Adjust all ETFs for splits and dividends and add an "a" at the end
for(ticker in my_etfs1) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text = expr))
}

# Combine all the returns into a matrix
all_returns1 = cbind(ClCl(VGTa),
								    ClCl(SOXXa),
                    ClCl(VLUEa),
                    ClCl(HDVa),
								    ClCl(AOAa))
# Omit first row of Nas
all_returns1 = as.matrix(na.omit(all_returns1))

#N = nrow(all_returns1)

# Compute the returns from the closing prices
pairs(all_returns1)

# Simulate many different possible futures by repeating the above block thousands of times
initial_wealth1 = 100000
sim1 = foreach(i = 1:5000, .combine = 'rbind') %do% {
	total_wealth1 = initial_wealth1
	weights1 = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings1 = weights1 * total_wealth1
	n_days = 20
	wealthtracker1 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today1 = resample(all_returns1, 1, orig.ids = FALSE)
		holdings1 = holdings1 + holdings1 * return.today1
		total_wealth1 = sum(holdings1)
		wealthtracker1[today] = total_wealth1
	}
	wealthtracker1
}
plot(wealthtracker1, type = 'l', main = "Wealthtracker 1", xlab = "Days 1 - 20", ylab = "Total Wealth")

# Profit/Loss histogram
hist(sim1[, n_days] - initial_wealth1, breaks = 20, main = paste("Profit/Loss 5000 runs"), xlab = ("P/L"))

# Profit/loss
print("The mean Profit for Portfolio 1 is:")
mean(sim1[, n_days])

print("The mean Loss for Portfolio 1 is:")
mean(sim1[, n_days] - initial_wealth1)

# 5% value at risk:
print("The VaR for Portfolio 1 is:")
quantile(sim1[, n_days] - initial_wealth1, prob = 0.05)
```
#### Portfolio 1 produces the lowest VaR of all portfolios

# *(b) Portfolio 2:------------------------------------------------------------*

## Portfolio 2 contains 3 categories: Large Market Cap, Alternative W/ Risk, and Global:

### Large Market Cap:
Large Cap Growth Equities ETFs: These ETFs invest in growth company stocks that are believed to have a large market capitalization size, generally with a market capitalization of $10 billion or more.

* SPY - SPDR S&P 500 ETF Trust
* XLC - Communication Services Select Sector SPDR Fund

### Alternative W/ Risk:
Alternative Energy Equities ETFs: These ETFs invest in alternative energy companies. The most popular and most common industry in this category is solar energy, although wind, hydroelectric, and geothermal energies are also represented here.

* GRID - First Trust Nasdaq Clean Edge Smart GRID Infrastructure Index

### Global:
Global Real Estate ETFs: These ETFs invest in real estate companies from all over the world. These ETFs can offer broad exposure to the industry, or can target specific subsectors such as residential property. In addition, some of these funds focus on the global ex-U.S. market, while others target a specific region or country.

* REET - iShares Global REIT ETF
* RWO - SPDR Dow Jones Global Real Estate ETF


```{r Portfolio 2 Bootstrap, echo=FALSE,  warning=FALSE, message=FALSE}

my_etfs2 = c("SPY", "XLC", "GRID", "REET", "RWO")
my_prices2 = getSymbols(my_etfs2, from = "2016-08-16")

# Adjust all ETFs for splits and dividends and add an "a" at the end
for(ticker in my_etfs2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text = expr))
}

# Combine all the returns into a matrix
all_returns2 = cbind(ClCl(SPYa),
								    ClCl(XLCa),
                    ClCl(GRIDa),
                    ClCl(REETa),
								    ClCl(RWOa))

# Omit first row of Nas
all_returns2 = as.matrix(na.omit(all_returns2))

#N = nrow(all_returns2)

# Compute the returns from the closing prices
pairs(all_returns2)

# Simulate many different possible futures by repeating the above block thousands of times
initial_wealth2 = 100000
sim2 = foreach(i = 1:5000, .combine = 'rbind') %do% {
	total_wealth2 = initial_wealth2
	weights2 = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings2 = weights2 * total_wealth2
	n_days = 20
	wealthtracker2 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today2 = resample(all_returns2, 1, orig.ids = FALSE)
		holdings2 = holdings2 + holdings2 * return.today2
		total_wealth2 = sum(holdings2)
		wealthtracker2[today] = total_wealth2
	}
	wealthtracker2
}
plot(wealthtracker2, type = 'l', main = "Wealthtracker 2", xlab = "Days 1 - 20", ylab = "Total Wealth")

# Profit/Loss histogram
hist(sim2[, n_days] - initial_wealth2, breaks = 20, main = paste("Profit/Loss 5000 runs"), xlab = ("P/L"))

# Profit/loss
print("The mean Profit for Portfolio 2 is:")
mean(sim2[, n_days])

print("The mean Loss for Portfolio 2 is:")
mean(sim2[, n_days] - initial_wealth2)

# 5% value at risk:
print("The VaR for Portfolio 2 is:")
quantile(sim2[, n_days] - initial_wealth2, prob = 0.05)
```
#### Portfolio 2 produces a slightly higher VaR than Portfolio 1.

# *(c) Portfolio 3:------------------------------------------------------------*

## Portfolio 3 contains 2 categories: Defensive and Complex:

### Defensive:
Energy Equities ETFs: An Inverse ETF (also known as a Short ETF or Bear ETF) is an ETF that is profitable for an investor during a market decline. These ETFs help investors defend against potential market downfalls, and can be used as a hedge against long positions during periods of market weakness.

* XLE - Energy Select Sector SPDR Fund
* XOP - SPDR S&P Oil & Gas Exploration & Production ETF
* IXC - iShares Global Energy ETF

### Complex:
MLPs ETFs: These ETFs invest in Master Limited Partnerships (MLPs). These companies are generally involved in the transportation, storage, and processing of energy commodities such as oil, natural gas, refined products, and natural gas liquids (NGLs). Funds in this category tend to have attractive dividend payouts.

* AMLP - Alerian MLP ETF
* EMLP - First Trust North American Energy Infrastructure Fund

```{r Portfolio 3 Bootstrap, echo = FALSE,  warning = FALSE, message = FALSE}

my_etfs3 = c("XLE", "XOP", "IXC", "AMLP", "EMLP")
my_prices3 = getSymbols(my_etfs3, from = "2016-08-16")

# Adjust all ETFs for splits and dividends and add an "a" at the end
for(ticker in my_etfs3) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text = expr))
}

# Combine all the returns into a matrix
all_returns3 = cbind(ClCl(XLEa),
								    ClCl(XOPa),
                    ClCl(IXCa),
                    ClCl(AMLPa),
								    ClCl(EMLPa))

# Omit first row of Nas
all_returns3 = as.matrix(na.omit(all_returns3))

#N = nrow(all_returns3)

# Compute the returns from the closing prices
pairs(all_returns3)

# Simulate many different possible futures by repeating the above block thousands of times
initial_wealth3 = 100000
sim3 = foreach(i = 1:5000, .combine = 'rbind') %do% {
	total_wealth3 = initial_wealth3
	weights3 = c(0.2, 0.2, 0.2, 0.2, 0.2)
	holdings3 = weights3 * total_wealth3
	n_days = 20
	wealthtracker3 = rep(0, n_days)
	for(today in 1:n_days) {
		return.today3 = resample(all_returns3, 1, orig.ids = FALSE)
		holdings3 = holdings3 + holdings3 * return.today3
		total_wealth3 = sum(holdings3)
		wealthtracker3[today] = total_wealth3
	}
	wealthtracker3
}
plot(wealthtracker3, type = 'l', main = "Wealthtracker 3", xlab = "Days 1 - 20", ylab = "Total Wealth")

# Profit/Loss histogram
hist(sim3[, n_days] - initial_wealth3, breaks = 20, main = paste("Profit/Loss 5000 runs"), xlab = ("P/L"))

# Profit/loss
print("The mean Profit for Portfolio 3 is:")
mean(sim3[, n_days])

print("The mean Loss for Portfolio 3 is:")
mean(sim3[, n_days] - initial_wealth3)

# 5% value at risk:
print("The VaR for Portfolio 3 is:")
quantile(sim3[, n_days] - initial_wealth3, prob = 0.05)
```
#### Portfolio 3 produces the highest VaR.

### Summary:
It was a bit surprising to discover that Portfolio 3 produced the highest VaR considering one category was defensive. The MLP ETFs can be difficult to diversify, but that space is growing as their investment strategies are becoming easier to understand. Some owners of MPL ETFS are seeing high dividend payouts, but the VaR of this portfolio is about double compared to Portfolio 1, and the mean profit returns a loss. 

This analysis would lead us to invest with Portfolio 1. Not only does it have the lowest VaR, it also returns the highest profit mean, with a mean loss that is always positive. The tech equities are considered somewhat risky because of that ever changing environment, but for now the reward profile looks to pay off.

Based off of this analysis, we would build a portfolio that is income focused, diverse, and includes some risk but is balanced out by value oriented low to moderate growth prospects.

# **4. Market Segmentation**

```{r Read in csv, echo=FALSE, include=FALSE, message=FALSE}

sm = read.csv("C:/Users/smkal/Desktop/ML_E/4_MS/social_marketing.csv", header = TRUE)
attach(sm)
#View(sm)

#take out spam column where sum is only 51, therefore not enough data
sm = subset(sm, select = -c(spam))
```


```{r Center and Scale, echo=FALSE, include=FALSE, message=FALSE}

# Center and scale the data, leaving out first column
X = sm[,2:36]
X = scale(X, center=TRUE, scale=TRUE)

# Extract the centers and scales from the re-scaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

We will use clustering to analyze the social marketing data provided by NutrientH20. The goal is to place the social media audience into small groups so that ads can be targeted more specifically. Because of the limited amount of information it provides, we will leave out the spam column for this analysis. Listed below are the 36 features that we will be working with:

```{r Show Features, echo=FALSE, message=FALSE}

names(sm[2:36])
```

### To begin the clustering process, we look for the best value for k, or number of clusters. First we will look at an elbow plot:

```{r Elbow Plot, echo=FALSE, message=FALSE, warning=FALSE}

# Elbow plot to look for K
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
    cluster_k = kmeans(X, k, nstart=50)
    cluster_k$tot.withinss
  }
plot(k_grid, SSE_grid)
```

### The elbow plot is not giving a definitive enough value for k, so we will try plotting the CH index:

```{r CH Index, echo=FALSE, message=FALSE, warning=FALSE}

# So, using CH index to look for K
N = nrow(sm)
CH_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(X, k, nstart=50)
  W = cluster_k$tot.withinss
  B = cluster_k$betweenss
  CH = (B/W)*((N-k)/(k-1))
  CH
}
plot(k_grid, CH_grid)
```

### Now we see that the best value for k should be somewhere around 4 or 5. After trying 3, 4, 5, and 6, 5 produces the best results and captures the best separation of features. Now we will look at the top 5 features within each cluster:

```{r Plot Clusters, echo=FALSE, include=FALSE, message=FALSE}

set.seed(5)
# Using kmeans++
clust_sm = kmeanspp(X, k=5, nstart=25)

plot(X, col = (clust_sm$cluster + 6), main = "K-means with K = 5", 
     xlab = "", ylab = "", pch = 20, cex = 2)

################################################################################
### These are the clusters:

#1 "chatter", "photo_sharing", "current_events", "college_uni", "health_nutrition"
sort(clust_sm$center[1,]*sigma + mu, decreasing = TRUE)

#2 "chatter", "photo_sharing", "cooking", "college_uni", "fashion"
sort(clust_sm$center[2,]*sigma + mu, decreasing = TRUE)

#3 "politics", "travel", "news", "chatter", "computers"
sort(clust_sm$center[3,]*sigma + mu, decreasing = TRUE)

#4 "sports_fandom", "religion", "food", "parenting", "chatter"
sort(clust_sm$center[4,]*sigma + mu, decreasing = TRUE)

#5 "health_nutrition", "personal_fitness", "chatter", "cooking", "outdoors"
sort(clust_sm$center[5,]*sigma + mu, decreasing = TRUE)
################################################################################
```

### The Cluster Top 5 Features breakdown is as follows:

#### Cluster 1:
* **"chatter", "photo_sharing", "current_events", "college_uni", "health_nutrition"**

#### Cluster 2:
* **"chatter", "photo_sharing", "cooking", "college_uni", "fashion"**

#### Cluster 3:
* **"politics", "travel", "news", "chatter", "computers"**

#### Cluster 4:
* **"sports_fandom", "religion", "food", "parenting", "chatter"**

#### Cluster 5:
* **"health_nutrition", "personal_fitness", "chatter", "cooking", "outdoors"**

### We now examine each cluster looking for further evidence to narrow our findings:

* **health_nutrition and politics scored the highest out of any cluster and as you can see below the two features are separated rather nicely. This clearly tells us that there are customers who would respond better to one of these categories, but not both.**
    
```{r Highest Scorers, echo=FALSE, message=FALSE}

### health_nutrition and politics score the highest out of any cluster
qplot(health_nutrition, politics, data=sm, col = factor(clust_sm$cluster))
```
\newpage

* **The plot below reveals that perhaps tweets that share photos are incorrectly classified as chatter. The chatter feature contains a lot of information so more precise classification could produce valuable results. For now, we suggest targeting chatter tweeters with photo_sharing content/ads.**

```{r Chatter, echo=FALSE, message=FALSE}

### photo_sharing tweets could be incorrectly classified as chatter
qplot(chatter, photo_sharing, data=sm, col = factor(clust_sm$cluster))
```
\newpage

* **In the plot below we see that gamers don't tweet about politics much, telling us that the two categories shouldn't be mixed when targeting consumers**

```{r Politics and Gaming, echo=FALSE, message=FALSE}

### politics and online_gaming rarely mix
qplot(politics, online_gaming, data=sm, col = factor(clust_sm$cluster))
```
\newpage

* **In both of the plots below, we see that users who tweet about family and religion typically don't tweet about adult content. Hoever, there are some interesting outliers, and it's concerning that family doesn't converge totally from the adult feature. These features should not mix well, and must always be considered because one misplaced ad could deter a lot of business.**   

```{r Family/Religion and Adult, echo=FALSE, message=FALSE}

# family and adult
qplot(family, adult, data=sm, col = factor(clust_sm$cluster))

# religion and adult
qplot(religion, adult, data=sm, col = factor(clust_sm$cluster))
```
\newpage

* **Below we see that targeting tweeters who are interested in both sports and religion may prove profitable.** 

```{r Sports and Religion, echo=FALSE, message=FALSE}

# sports and religion often together
qplot(sports_fandom, religion, data=sm, col = factor(clust_sm$cluster))
```


```{r Within and Between, echo=FALSE, include=FALSE, message=FALSE}

# within-cluster average distances
clust_sm$withinss
sum(clust_sm$withinss)

# between cluster average distance
clust_sm$betweenss
```

### Summary:
This analysis is just beginning to tap into what could be discovered with this data set. We believe the biggest takeway is to more accurately classify every single tweet rather than just dropping a lot of them into a general category like chatter or uncategorized. There is a lot of potentially valuable information lost if the categorization process is not done with complete precision. That being said, we were able to find 5 distinct clusters allowing for more targeted ads and information gathering. 

# **5. Author attribution**

Each story is processed to remove apostrophes, convert to lowercase, change non-alphanumeric characters to spaces, and so on. It is then tokenized into a list of words, and added to a list of lists of tokens, called all_stories. We keep track of the author names and whether that story was in the train or test set separately.

all_stories is used to create a Corpus object, which we remove all useless stopwords from (as defined by the tm package). We then convert the Corpus object to a Document Term Matrix, which keeps track of how often each word appeared in each story across the entire corpus. We remove all sparse terms here (terms that appear in less than 5% of stories). The TF-IDF weights of each word is now calculated and used to build the dataframe we'll be working with.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

strip.text <- function(txt) {
  # remove apostrophes (so "don't" -> "dont", "Jane's" -> "Janes", etc.)
  txt <- gsub("'","",txt)
  # convert to lowercase
  txt <- tolower(txt)
  # change other non-alphanumeric characters to spaces
  txt <- gsub("[^a-z0-9]"," ",txt)
  # change digits to #
  txt <- gsub("[0-9]+","#",txt)
  # split and make one vector
  txt <- unlist(strsplit(txt," "))
  # remove empty words
  txt <- txt[txt != ""]
  return(txt)
}

read.directory <- function(dirname,verbose=FALSE) {
  stories = list()
  filenames = dir(dirname,full.names=TRUE) #Get all file names
  for (i in 1:length(filenames)) {
    if(verbose) {
      print(filenames[i])
    }
    stories[[i]] = strip.text(read_file(filenames[i])) #For each file, read the text, convert to a vector, and dump into a list object
  }
  return(stories)
}
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

authors = list()
train_labels = list()
all_stories = list()

dirs = list.dirs(path = "C:/Users/smkal/Desktop/ML_2nd Half/data/ReutersC50/C50train", full.names = TRUE, recursive = FALSE)
for(d in dirs){
  stories = read.directory(d)
  for (i in 1:length(stories)){
    all_stories[length(all_stories) + 1] = stories[i]
    authors[length(authors) + 1] = d
    train_labels[length(train_labels) + 1] = 1
  }
}
dirs = list.dirs(path = "C:/Users/smkal/Desktop/ML_2nd Half/data/ReutersC50/C50test", full.names = TRUE, recursive = FALSE)
for(d in dirs){
  stories = read.directory(d)
  for (i in 1:length(stories)){
    all_stories[length(all_stories) + 1] = stories[i]
    authors[length(authors) + 1] = d
    train_labels[length(train_labels) + 1] = 0
  }
}

all_stories = Corpus(VectorSource(all_stories))
all_stories = tm_map(all_stories, content_transformer(removeWords), stopwords("en"))
dtm = DocumentTermMatrix(all_stories)
dtm = removeSparseTerms(dtm, 0.95)
tfidf = weightTfIdf(dtm)
tfidf = as.matrix(tfidf)
df = data.frame(as.matrix(tfidf), stringsAsFactors=FALSE)
```

### We remove all columns that are all zeroes (they would cause issues with the PCA calculations), then run a PCA analysis on our dataframe. From looking at the summary we can see that with 158 columns we can encode 50% of the original information.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

scrub_cols = which(colSums(df) == 0)
pca = prcomp(df[,-scrub_cols], scale=TRUE)
options(max.print=1000000)
summary(pca)
```

For our model we decided to go with a random forest. Maybe certain authors use words that no other author does - for example a sports columnist will use the word 'umpire' frequently but a non-sports columnist would never use it. A decision tree is perfect for using such clear-cut divisions in the data to separate classes, and a random forest might be able to handle complexity of the dataset better than a simple decision tree.

We also tried training our model with several different datasets: the raw TF-IDF matrix, 10 PCA columns, 158 PCA columns, and all PCA columns, to see what did the best and how long it took.

Unsurprisingly, datasets with more features are more accurate but are much slower. Interestingly, the all PCA columns dataset is outperformed by both the TF-IDF matrix and the 158 PCA columns dataset. Likely it encodes a lot of useless information in it's later columns that only confuse the model.

We can see that using 158 PCA columns has almost as good an accuracy as the TF-IDF matrix but takes less than half the time to train. What model is best ultimately depends on what the use case is. If accuracy is the most important, use the TF-IDF matrix without PCA reduction. If you care how long it takes, use some PCA columns, with how many you use depending on how long you can afford for the algorithm to run. Just don't use too many, or you'll start getting worse accuracies.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#Convert authors from file paths to just author names
authors = str_extract(authors, "[^/]*$")

for(model_df in list(df, data.frame(pca$x[, 0:10]), data.frame(pca$x[, 0:158]), data.frame(pca$x))){
  model_df$Train = as.matrix(unlist(train_labels))
  model_df$Author = as.matrix(unlist(authors))
  
  train = model_df[model_df$Train == 1,]
  test = model_df[model_df$Train == 0,]
  
  train$Author = factor(train$Author) 
  test$Author = factor(test$Author) 
  
  model = randomForest(Author ~ ., data=train)
  prediction = predict(model, test)
  num_right = sum(as.numeric(as.character(prediction) == as.character(test$Author)))
  
  print(100.0 * num_right / nrow(test))
}
```

# **6. Association Rule Mining**

### We first read in a large list of baskets as individual transactions and plot the top 20 most frequently occurring items:

```{r Read in txt file as transactions, echo=FALSE, message=FALSE, warning=FALSE}

#Read in text file as transactions
gr_trans = read.transactions("C:/Users/smkal/Desktop/ML_E/6_ARM/groceries.txt", sep=",")

#itemLabels(gr_trans)
#?RColorBrewer

# plot the 20 most frequent items
itemFrequencyPlot(gr_trans, topN=20, cex.names=1, type="absolute", col=brewer.pal(12,'Set3'), 
                  main="Absolute Item Frequency Plot")
```

### Now that the data is in transaction form, we can use the A-Priori Algorithm to create rules. Listed below are the first ten rules:

```{r Apriori, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}

# A-Priori Algorithm
rules = apriori(gr_trans, parameter=list(supp=0.001, conf=0.6, minlen=2, maxlen=10, target="rules"))

old_sub_rules = which(colSums(is.subset(rules, rules)) > 1)
new_rules = rules[-old_sub_rules]
#inspect(new_rules[1:10])
```


```{r Display 10 Rules, echo=FALSE, message=FALSE, warning=FALSE}

#display first 10 rules
arules::inspect(new_rules[1:10])
```

### Now we analyze a couple of specific rules. Our examples here attempt to predict what customers buy before and after buying soda and shopping bags. Below you can see a few of these predictions:

```{r Analyze Rules by Item, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}

### soda
# let's see what shoppers buy after buying soda
soda_rules_lhs = apriori(gr_trans, parameter=list(supp=0.001, conf=0.2, minlen=2, maxlen=6), 
                         appearance=list(lhs="soda", default="rhs"))

# let's see what shoppers buy before buying soda
soda_rules_rhs = apriori(gr_trans, parameter=list(supp=0.001, conf=0.7, minlen=2, maxlen=6), 
                         appearance=list(default="lhs", rhs="soda"))

### shopping bags
# let's see what shoppers buy after buying shopping bags
bags_rules_lhs = apriori(gr_trans, parameter=list(supp=0.001, conf=0.1, minlen=2, maxlen=6), 
                         appearance=list(lhs="shopping bags", default="rhs"))

# let's see what shoppers buy before buying shopping bags
bags_rules_rhs = apriori(gr_trans, parameter=list(supp=0.001, conf=0.4, minlen=2, maxlen=6), 
                         appearance=list(default="lhs", rhs="shopping bags"))

```


```{r Soda and Bags Rules, echo=FALSE, message=FALSE, warning=FALSE}

arules::inspect(soda_rules_lhs[1:2])
arules::inspect(soda_rules_rhs[1:2])
arules::inspect(bags_rules_lhs[1:5])
arules::inspect(bags_rules_rhs[1:5])
```

### Now we will visualize the rules below

#### Scatter plot with conditional subset:
Rules with confidence values above 0.75 are plotted here. The lift, or "interestingness" measures here indicate that these rules have a complementary effect. If an item is purchased, another item is likely to be purchased with it, not instead of it. There is a clear outlier here. This could be something like milk and cereal, which is often purchased together. All of these points have high confidence, but don't come into play very often with this data set.

```{r Visualize rules, echo=FALSE, message=FALSE, warning=FALSE}

# visualize rules with conditional subset
sub1 = new_rules[quality(new_rules)$confidence > 0.75]
plot(sub1, measure=c("support", "lift"), shading="confidence")
```
\newpage

#### Graph-based visualization:
Here we see some interesting relationships. Clearly different types of alcohol are bought together. 

```{r Graph-Based Visualization, echo=FALSE, message=FALSE, warning=FALSE}

# graph-based visualization with conditional subset
sub2 = subset(new_rules, subset=confidence > 0.9 & support > 0.001)
plot(head(sub2, 10, by='lift'), method='graph')
```
\newpage

#### Two-key plot:
The support for order 3 varies widely here. 

```{r Two-Key Plot, echo=FALSE, message=FALSE, warning=FALSE}

# two-key plot
plot(new_rules, method='two-key plot')
```
\newpage

```{r Interactive Scatter Plot, echo=FALSE, message=FALSE, warning=FALSE}

#### Interactive scatter plot:
#It's no surprise that ham, processed cheese, and white bread are often bought together which you can see at the bottom #right red dot. 

# Had to exclude this plot for knit to pdf

# interactive scatter plot
#plot(new_rules, engine="plotly")
```


```{r Interactive Network Graph, echo=FALSE, message=FALSE, warning=FALSE}

#### Interactive network graph:
#Other vegetables are at the heart of a lot shopping lists it seems!


# Had to exclude this plot for knit to pdf

# interactive network graph with subset
#sub3 = head(new_rules, n=10, by="confidence")
#plot(sub3, method="graph",  engine="htmlwidget")
```

#### Parallel coordinate plot:
Here we can see that yogurt and root vegetables purchases are a likely occurrence after buying at least one of a number of items.

```{r Parallel Coordinate Plot, echo=FALSE, message=FALSE, warning=FALSE}

# parallel coordinate plot
sub4 = head(sub1, n=10, by="lift")
plot(sub4, method="paracoord")
```

### Summary:
Vegetables and whole milk are a major centerpiece in most grocery shopping transactions. Wine, liquor, and beer are bought together often. If customers buy beer, they're also likely to buy bags, perhaps to keep their plans under wraps. We can also predict with confidence that if customers buy coffee or other miscellaneous beverages, they will also buy soda. There are plenty of other rules that were created by the A-Priori Algorithm, and one could hone quite an advertising strategy based on this analysis alone.
